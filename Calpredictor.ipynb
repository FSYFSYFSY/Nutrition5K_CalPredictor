{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T14:08:46.194094Z",
     "iopub.status.busy": "2025-10-20T14:08:46.193901Z",
     "iopub.status.idle": "2025-10-20T14:08:57.343413Z",
     "shell.execute_reply": "2025-10-20T14:08:57.342762Z",
     "shell.execute_reply.started": "2025-10-20T14:08:46.194068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "#from sklearn.model_selection import KFold \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T14:08:57.345911Z",
     "iopub.status.busy": "2025-10-20T14:08:57.345508Z",
     "iopub.status.idle": "2025-10-20T14:08:57.358045Z",
     "shell.execute_reply": "2025-10-20T14:08:57.357299Z",
     "shell.execute_reply.started": "2025-10-20T14:08:57.345890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 if_train = True,\n",
    "                 data_dir = \"train\",\n",
    "                 color_dir = \"color\",\n",
    "                 depth_dir = \"depth_raw\",\n",
    "                 rgb_name = \"rgb.png\",\n",
    "                 depth_name=\"depth_raw.png\",\n",
    "                 csv_name = \"nutrition5k_train.csv\",\n",
    "                 transform = None, \n",
    "                 ):\n",
    "        self.root = root\n",
    "        self.data = self.root / data_dir\n",
    "        self.color_dir  = self.data / color_dir\n",
    "        self.depth_dir = self.data / depth_dir\n",
    "        self.rgb_name = rgb_name\n",
    "        self.depth_name = depth_name\n",
    "        self.if_train = if_train\n",
    "        \n",
    "\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.color_jitter = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
    "    \n",
    "\n",
    "        if self.if_train:\n",
    "            df = pd.read_csv(self.root / csv_name)\n",
    "            self.id2cal = {str(r[\"ID\"]) : float(r[\"Value\"]) for _, r in df.iterrows()}\n",
    "\n",
    "        rgb_paths = sorted((p / rgb_name for p in self.color_dir.glob(\"dish_*\") if (p / rgb_name).exists()))\n",
    "        if not rgb_paths:\n",
    "            raise RuntimeError(f\"Found 0 images in {self.color_dir}.\")\n",
    "        \n",
    "        self.samples = []\n",
    "        if self.if_train:\n",
    "            for rgb_path in rgb_paths:\n",
    "                dish_id = rgb_path.parent.name\n",
    "                if dish_id not in self.id2cal:\n",
    "                    print(f\"Warning: {dish_id} not found in CSV.\")\n",
    "                depth_path = self.depth_dir / dish_id / self.depth_name \n",
    "                self.samples.append((rgb_path, depth_path, self.id2cal[dish_id]))\n",
    "        else:\n",
    "            for rgb_path in rgb_paths:\n",
    "                dish_id = rgb_path.parent.name\n",
    "                depth_path = self.depth_dir / dish_id / self.depth_name \n",
    "                self.samples.append((rgb_path, depth_path, None))\n",
    "                \n",
    "    def __len__(self):\n",
    "            return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            rgb_path, depth_path, cal = self.samples[idx]\n",
    "            rgb = Image.open(rgb_path).convert(\"RGB\")\n",
    "            depth = Image.open(depth_path).convert(\"L\")\n",
    "\n",
    "            # Data enhencement\n",
    "            # Data enhancement\n",
    "            if self.if_train:\n",
    "                # 1. Center crop to 480x480\n",
    "                rgb = TF.center_crop(rgb, (480, 480))\n",
    "                depth = TF.center_crop(depth, (480, 480))\n",
    "\n",
    "                # 2. Random 90-degree rotation\n",
    "                k = random.randint(0, 3)  # 0: 0°, 1: 90°, 2: 180°, 3: 270°\n",
    "                if k > 0:\n",
    "                    rgb = TF.rotate(rgb, angle=90 * k, interpolation=TF.InterpolationMode.BILINEAR)\n",
    "                    depth = TF.rotate(depth, angle=90 * k, interpolation=TF.InterpolationMode.NEAREST)\n",
    "\n",
    "            else:\n",
    "                # Apply same crop for validation/test\n",
    "                rgb = TF.center_crop(rgb, (480, 480))\n",
    "                depth = TF.center_crop(depth, (480, 480))\n",
    "\n",
    "            \n",
    "            rgb = self.to_tensor(rgb)\n",
    "            depth = self.to_tensor(depth)\n",
    "            rgb = self.normalize(rgb)\n",
    "\n",
    "            if self.if_train:\n",
    "                cal = torch.tensor(cal, dtype=torch.float32)\n",
    "                return rgb, depth, cal\n",
    "            else:\n",
    "                return rgb, depth\n",
    "\n",
    "        except (UnidentifiedImageError, FileNotFoundError, Exception) as e:\n",
    "            print(f\"waring: index {idx} sample processing fail, will be taken place by original sample。error: {e}\")\n",
    "            new_idx = random.randint(0, len(self) - 1)\n",
    "            return self.__getitem__(new_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T14:08:57.359256Z",
     "iopub.status.busy": "2025-10-20T14:08:57.358935Z",
     "iopub.status.idle": "2025-10-20T14:08:57.386926Z",
     "shell.execute_reply": "2025-10-20T14:08:57.386283Z",
     "shell.execute_reply.started": "2025-10-20T14:08:57.359237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/lhg45/Desktop/COMP90086/PJ/Nutrition5K\n"
     ]
    }
   ],
   "source": [
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T14:08:57.388006Z",
     "iopub.status.busy": "2025-10-20T14:08:57.387643Z",
     "iopub.status.idle": "2025-10-20T14:09:21.301306Z",
     "shell.execute_reply": "2025-10-20T14:09:21.300522Z",
     "shell.execute_reply.started": "2025-10-20T14:08:57.387975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_set = MyDataset(root=Path.cwd(), if_train=True, transform=False)\n",
    "test_set = MyDataset(root=Path.cwd(), data_dir=\"test\", if_train=False, transform=False)\n",
    "\n",
    "dataset_size = len(train_set)\n",
    "val_ratio = 0.1\n",
    "val_size = int(dataset_size * val_ratio)\n",
    "train_size = dataset_size - val_size\n",
    "train_subset, val_subset = random_split(train_set, [train_size, val_size])\n",
    "\n",
    "#torch.manual_seed(42) \n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=8, shuffle=True,\n",
    "                          num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=8, shuffle=False,\n",
    "                        num_workers=8, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set,  batch_size=8, shuffle=False,\n",
    "                          num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T14:09:21.302408Z",
     "iopub.status.busy": "2025-10-20T14:09:21.302148Z",
     "iopub.status.idle": "2025-10-20T14:09:21.328908Z",
     "shell.execute_reply": "2025-10-20T14:09:21.328117Z",
     "shell.execute_reply.started": "2025-10-20T14:09:21.302380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def conv_block(c_in, c_out, k=3, s=1, p=1, bias=False):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(c_in, c_out, kernel_size=k, stride=s, padding=p, bias=bias),\n",
    "        nn.BatchNorm2d(c_out),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class PreActResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # BN -> ReLU -> Conv\n",
    "        self.preact_conv1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) \n",
    "        )\n",
    "        \n",
    "        self.preact_conv2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x) \n",
    "        out = self.preact_conv1(x)\n",
    "        out = self.preact_conv2(out)\n",
    "        out += identity          \n",
    "        return out\n",
    "    \n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, dim_model: int):\n",
    "        super().__init__()\n",
    "        self.attn_fc = nn.Linear(dim_model, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):  # x: (B, num_tokens, dim_model)\n",
    "        attn_scores = self.attn_fc(x)              # (B, num_tokens, 1)\n",
    "        attn_weights = F.softmax(attn_scores, dim=1)\n",
    "        pooled = torch.sum(attn_weights * x, dim=1)\n",
    "        return pooled\n",
    "    \n",
    "class TransformerHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int = 512,\n",
    "        num_tokens: int = 8,\n",
    "        dim_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.dim_model  = dim_model\n",
    "\n",
    "        self.token_embed = nn.Linear(in_features, num_tokens * dim_model)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_tokens, dim_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=dim_model,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=2\n",
    "        )\n",
    "\n",
    "        self.attention_pool = AttentionPooling(dim_model)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(512),              \n",
    "            nn.Linear(512, 256),           \n",
    "            nn.GELU(),                      \n",
    "            nn.Dropout(0.2),                \n",
    "            nn.Linear(256, 64),             \n",
    "            nn.ReLU(),                      \n",
    "            nn.Dropout(0.1),                \n",
    "            nn.Linear(64, 1)                \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        tokens = self.token_embed(x)                     # (B, num_tokens * dim_model)\n",
    "        tokens = tokens.view(B, self.num_tokens, self.dim_model)\n",
    "        tokens = tokens + self.pos_embed\n",
    "        tokens = self.transformer(tokens)                # (B, num_tokens, dim_model)\n",
    "        feat = self.attention_pool(tokens)               # (B, dim_model)\n",
    "        return self.fc(feat).squeeze(1)\n",
    "    \n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_features: int = 128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(in_features),\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc(x).squeeze(1)\n",
    "\n",
    "    \n",
    "    \n",
    "class BFPFusion(nn.Module):\n",
    "        def __init__(self, out_channels=256, refine_level=2):\n",
    "            super().__init__()\n",
    "            self.refine_level = refine_level\n",
    "            self.refine_conv = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "        def forward(self, feats):\n",
    "\n",
    "            target_size = feats[self.refine_level].shape[2:]\n",
    "            resized = [F.interpolate(f, size=target_size, mode=\"nearest\") for f in feats]\n",
    "            fused = torch.stack(resized, dim=0).mean(dim=0)  # (B, C, H, W)\n",
    "\n",
    "            fused = self.refine_conv(fused)\n",
    "\n",
    "            return fused\n",
    "\n",
    "\n",
    "class RGBBranch(nn.Module):\n",
    "    def __init__(self, in_ch=3):\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, 16, kernel_size=5, stride=2, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        # 四个stage，对应C2~C5\n",
    "        self.layer1 = nn.Sequential(\n",
    "            PreActResidualBlock(16, 32, stride=2),\n",
    "            PreActResidualBlock(32, 32, stride=1)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            PreActResidualBlock(32, 64, stride=2),\n",
    "            PreActResidualBlock(64, 64, stride=1)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            PreActResidualBlock(64, 128, stride=2),\n",
    "            PreActResidualBlock(128, 128, stride=1)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            PreActResidualBlock(128, 256, stride=2),\n",
    "            PreActResidualBlock(256, 256, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        c2 = self.layer1(x)  # 1/4\n",
    "        c3 = self.layer2(c2) # 1/8\n",
    "        c4 = self.layer3(c3) # 1/16\n",
    "        c5 = self.layer4(c4) # 1/32\n",
    "        return [c2, c3, c4, c5]\n",
    "\n",
    "\n",
    "class DepthBranch(RGBBranch):\n",
    "    def __init__(self, in_ch=1):\n",
    "        super().__init__(in_ch)\n",
    "\n",
    "\n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels=[32,64,128,256], out_channels=128):\n",
    "        super().__init__()\n",
    "        self.lateral_convs = nn.ModuleList([\n",
    "            nn.Conv2d(c, out_channels, kernel_size=1) for c in in_channels\n",
    "        ])\n",
    "        self.output_convs = nn.ModuleList([\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1) for _ in in_channels\n",
    "        ])\n",
    "\n",
    "    def forward(self, features):\n",
    "        c2, c3, c4, c5 = features\n",
    "        p5 = self.lateral_convs[3](c5)\n",
    "        p4 = self.lateral_convs[2](c4) + F.interpolate(p5, size=c4.shape[2:], mode=\"nearest\")\n",
    "        p3 = self.lateral_convs[1](c3) + F.interpolate(p4, size=c3.shape[2:], mode=\"nearest\")\n",
    "        p2 = self.lateral_convs[0](c2) + F.interpolate(p3, size=c2.shape[2:], mode=\"nearest\")\n",
    "\n",
    "        p2 = self.output_convs[0](p2)\n",
    "        p3 = self.output_convs[1](p3)\n",
    "        p4 = self.output_convs[2](p4)\n",
    "        p5 = self.output_convs[3](p5)\n",
    "\n",
    "        return [p2, p3, p4, p5]\n",
    "    \n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.channel_att = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.spatial_att = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, kernel_size=7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        ca = self.channel_att(x)\n",
    "        x = x * ca\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        sa = self.spatial_att(torch.cat([avg_out, max_out], dim=1))\n",
    "        x = x * sa\n",
    "        return x\n",
    "\n",
    "\n",
    "class RGBDNet(nn.Module):\n",
    "    def __init__(self, num_tokens=16, dim_model=512, nhead=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rgb   = RGBBranch(3)\n",
    "        self.depth = DepthBranch(1)\n",
    "\n",
    "        fusion_dim = 128\n",
    "\n",
    "        self.fpn_rgb   = FPN([32,64,128,256], fusion_dim)\n",
    "        self.fpn_depth = FPN([32,64,128,256], fusion_dim)\n",
    "        self.bfp = BFPFusion(out_channels=fusion_dim, refine_level=2)\n",
    "\n",
    "        self.cbam = CBAM(channels=fusion_dim)\n",
    "        self.head = MLPHead(in_features=fusion_dim)\n",
    "\n",
    "        '''\n",
    "        self.head = TransformerHead(\n",
    "            in_features=fusion_dim,\n",
    "            num_tokens=num_tokens,\n",
    "            dim_model=dim_model,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        '''\n",
    "\n",
    "    def forward(self, rgb, depth):\n",
    "        f_rgb   = self.rgb(rgb)     # [c2,c3,c4,c5]\n",
    "        f_depth = self.depth(depth)\n",
    "\n",
    "        p_rgb   = self.fpn_rgb(f_rgb)     # [p2,p3,p4,p5]\n",
    "        p_depth = self.fpn_depth(f_depth)\n",
    "\n",
    "        fused = [r + d for r, d in zip(p_rgb, p_depth)]\n",
    "        out = self.bfp(fused)\n",
    "        #out = self.cbam(out)\n",
    "\n",
    "        out_vec = F.adaptive_avg_pool2d(out, 1).flatten(1)\n",
    "        output = self.head(out_vec)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-20T14:10:55.500Z",
     "iopub.execute_input": "2025-10-20T14:09:21.329789Z",
     "iopub.status.busy": "2025-10-20T14:09:21.329538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detect 1 GPU avaliable。\n",
      "Start epoch: 1\n",
      "Epoch [1/100] | Train Loss: 59971.7185, Val Loss: 24684.7790, Train SMAPE: 102.852%, Val MAPE: 67.438%\n",
      "Start epoch: 2\n",
      "Epoch [2/100] | Train Loss: 32091.5080, Val Loss: 30632.5761, Train SMAPE: 70.748%, Val MAPE: 65.278%\n",
      "Start epoch: 3\n",
      "Epoch [3/100] | Train Loss: 28321.9925, Val Loss: 25133.7538, Train SMAPE: 64.502%, Val MAPE: 63.107%\n",
      "Start epoch: 4\n",
      "Epoch [4/100] | Train Loss: 27105.2035, Val Loss: 20334.5119, Train SMAPE: 64.199%, Val MAPE: 61.326%\n",
      "Start epoch: 5\n",
      "Epoch [5/100] | Train Loss: 26305.0285, Val Loss: 16376.5678, Train SMAPE: 63.230%, Val MAPE: 55.928%\n",
      "Start epoch: 6\n",
      "Epoch [6/100] | Train Loss: 26680.3582, Val Loss: 16305.8887, Train SMAPE: 62.826%, Val MAPE: 57.591%\n",
      "Start epoch: 7\n",
      "Epoch [7/100] | Train Loss: 24558.6700, Val Loss: 15347.6377, Train SMAPE: 63.141%, Val MAPE: 54.883%\n",
      "Start epoch: 8\n",
      "Epoch [8/100] | Train Loss: 24765.5336, Val Loss: 15745.6844, Train SMAPE: 62.693%, Val MAPE: 55.303%\n",
      "Start epoch: 9\n",
      "Epoch [9/100] | Train Loss: 25342.1251, Val Loss: 17451.9574, Train SMAPE: 61.408%, Val MAPE: 57.260%\n",
      "Start epoch: 10\n",
      "Epoch [10/100] | Train Loss: 24196.4368, Val Loss: 14201.6043, Train SMAPE: 60.753%, Val MAPE: 53.383%\n",
      "Start epoch: 11\n",
      "Epoch [11/100] | Train Loss: 24910.1607, Val Loss: 17600.5667, Train SMAPE: 60.627%, Val MAPE: 57.780%\n",
      "Start epoch: 12\n",
      "Epoch [12/100] | Train Loss: 22787.9679, Val Loss: 16662.3411, Train SMAPE: 60.415%, Val MAPE: 57.365%\n",
      "Start epoch: 13\n",
      "Processed 2970 images\r"
     ]
    }
   ],
   "source": [
    "class DualThresholdEarlyStopping:\n",
    "    def __init__(self, train_threshold=7000.0, val_threshold=7000.0):\n",
    "        self.train_threshold = train_threshold\n",
    "        self.val_threshold = val_threshold\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, val_loss):\n",
    "        if train_loss < self.train_threshold and val_loss < self.val_threshold:\n",
    "            self.early_stop = True\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"detect {torch.cuda.device_count()} GPU avaliable。\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"No GPU detect, runing on cpu\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = RGBDNet()  \n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"using multipule GPU\")\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6 )\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=5e-5, weight_decay = 5e-4 )\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "criterion = nn.MSELoss()\n",
    "early_stopper = DualThresholdEarlyStopping(train_threshold=9000, val_threshold=7500)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_smapes, val_smapes = [], []\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, smape_sum, n_batches = 0.0, 0.0, 0\n",
    "    print(\"Start epoch: \" + str(epoch+1))\n",
    "    image_count = 0\n",
    "\n",
    "    for rgb, depth, cal in train_loader:\n",
    "        rgb, depth, cal = rgb.to(device), depth.to(device), cal.to(device)\n",
    "        pred = model(rgb, depth)\n",
    "        loss = criterion(pred, cal)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * rgb.size(0)\n",
    "        smape_batch = torch.mean( 2 * torch.abs(pred - cal) / (torch.abs(pred) + torch.abs(cal) + 1e-8)).item()\n",
    "        smape_sum += smape_batch\n",
    "        n_batches += 1\n",
    "\n",
    "        image_count += rgb.size(0)\n",
    "        print(f\"Processed {image_count} images\", end='\\r')\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_smape = smape_sum / n_batches\n",
    "    train_losses.append(train_loss)\n",
    "    train_smapes.append(train_smape)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, smape_sum, n_batches = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for rgb, depth, cal in val_loader:\n",
    "            rgb, depth, cal = rgb.to(device), depth.to(device), cal.to(device)\n",
    "            pred = model(rgb, depth)\n",
    "            loss = criterion(pred, cal)\n",
    "            val_loss += loss.item() * rgb.size(0)\n",
    "            smape_batch = torch.mean( 2 * torch.abs(pred - cal) / (torch.abs(pred) + torch.abs(cal) + 1e-8)).item()\n",
    "            smape_sum += smape_batch\n",
    "            n_batches += 1\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_smape = smape_sum / n_batches\n",
    "    val_losses.append(val_loss)\n",
    "    val_smapes.append(val_smape)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Train SMAPE: {train_smape*100:.3f}%, Val MAPE: {val_smape*100:.3f}%\")\n",
    "\n",
    "    early_stopper(train_loss, val_loss)\n",
    "    if early_stopper.early_stop:\n",
    "        print(\"✅ Early stopping: both train and val loss thresholds met.\")\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-20T14:10:55.501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_path = \"checkpoints/FFN_BFN_epoch100_10_25.pth\" \n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model weights saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-20T14:10:55.501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('L2 Huber Loss Curve'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot([m*100 for m in train_smapes], label='Train SMAPE')\n",
    "plt.plot([m*100 for m in val_smapes], label='Val SMAPE')\n",
    "plt.title('SMAPE Curve'); plt.xlabel('Epoch'); plt.ylabel('SMAPE (%)'); plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-20T14:10:55.501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "submission_df = pd.DataFrame({\"ID\": [], \"Value\": []})\n",
    "with torch.no_grad():\n",
    "    for rgb, depth in test_loader:\n",
    "        rgb, depth = rgb.to(device), depth.to(device)\n",
    "        pred = model(rgb, depth)\n",
    "        predictions.extend(pred.cpu().numpy())\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": [f\"dish_{3300+i:04d}\" for i in range(1, len(predictions)+1)],\n",
    "    \"Value\": [0 if val < 5 else val for val in predictions]\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-20T14:10:55.501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_filepath = \"submission.csv\"\n",
    "\n",
    "submission_df.to_csv(output_filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 13740179,
     "sourceId": 115075,
     "sourceType": "competition"
    },
    {
     "datasetId": 8536190,
     "sourceId": 13448066,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 269863585,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (Pytorch2)",
   "language": "python",
   "name": "pytorch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
